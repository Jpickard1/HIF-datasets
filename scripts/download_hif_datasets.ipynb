{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ced1f8d-26c0-432d-bb98-2d31436bd32c",
   "metadata": {},
   "source": [
    "# Download a HIF Dataset from GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7448268e-116f-440d-841f-2f5b6d0abf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import fastjsonschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c311170-5c0a-42eb-8450-e0235db7b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hif_dataset(dataset_name, download_path):\n",
    "    # Step 1: get list of all HIF dataset names\n",
    "    hif_dataset_files = get_hif_datasets()\n",
    "    \n",
    "    # Step 2: Check if dataset exists and determine file pattern\n",
    "    dataset_files = find_dataset_files(hif_dataset_files, dataset_name)\n",
    "    \n",
    "    # Step 3: Download each file separately\n",
    "    for file_name in dataset_files:\n",
    "        download_hif_file(file_name, download_path=download_path)\n",
    "    \n",
    "    # Step 4: Join the files together\n",
    "    if len(dataset_files) > 1:\n",
    "        rejoin_files(dataset_name, directory=download_path)\n",
    "    dataset_file = os.path.join(download_path, dataset_name + \".hif\")\n",
    "    \n",
    "    # Step 5: Validate the final file to the HIF schma\n",
    "    validate_hif_schema(dataset_file)\n",
    "\n",
    "def get_file_size_mb(filepath):\n",
    "    \"\"\"Get file size in MB\"\"\"\n",
    "    return os.path.getsize(filepath) / (1024 * 1024)\n",
    "\n",
    "def get_hif_datasets():\n",
    "    api_url = \"https://api.github.com/repos/Jpickard1/HIF-datasets/contents/datasets\"\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        file_list = [f['name'] for f in files if f['type'] == 'file']\n",
    "    else:\n",
    "        file_list = []\n",
    "    return file_list\n",
    "\n",
    "def find_dataset_files(hif_dataset_files, dataset_name):\n",
    "    # Pattern 1: Single file (dataset_name.hif)\n",
    "    single_file_pattern = f\"{dataset_name}.hif\"\n",
    "    \n",
    "    # Pattern 2: Multi-file (dataset_name_1of5.hif, dataset_name_2of5.hif, etc.)\n",
    "    multi_file_pattern = re.compile(rf\"{re.escape(dataset_name)}_(\\d+)of(\\d+)\\.hif\")\n",
    "    \n",
    "    # Check for single file\n",
    "    single_file_found = None\n",
    "    for file in hif_dataset_files:\n",
    "        if file == single_file_pattern:\n",
    "            return [file]\n",
    "    \n",
    "    # Check for multi-file pattern\n",
    "    multi_files = []\n",
    "    for file in hif_dataset_files:\n",
    "        match = multi_file_pattern.match(file)\n",
    "        if match:\n",
    "            multi_files.append(file)\n",
    "\n",
    "    if len(multi_files) > 0:\n",
    "        return multi_files\n",
    "    else:\n",
    "        print(f\"No HIF dataset found matching {dataset_name}. Please check the dataset name and spelling.\")\n",
    "        return None\n",
    "\n",
    "def download_hif_file(file, download_path):\n",
    "    url = \"https://raw.githubusercontent.com/Jpickard1/HIF-datasets/main/datasets/\" + file\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_path, file), \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"File {file} downloaded successfully\")\n",
    "    else:\n",
    "        print(f\"Failed to download file: {response.status_code}\")\n",
    "\n",
    "def rejoin_files(base_name, directory=\".\"):\n",
    "    \"\"\"Rejoin split files back into original file\"\"\"\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    # Find all chunks for this base name\n",
    "    pattern = f\"{base_name}_*of*.hif\"\n",
    "    chunk_files = list(directory.glob(pattern))\n",
    "    \n",
    "    if not chunk_files:\n",
    "        print(f\"No chunk files found for {base_name}\")\n",
    "        return\n",
    "    \n",
    "    # Sort chunks by number\n",
    "    def get_chunk_number(filename):\n",
    "        # Extract number from filename like \"name_1of3.hif\"\n",
    "        stem = filename.stem\n",
    "        if \"_\" in stem and \"of\" in stem:\n",
    "            try:\n",
    "                number_part = stem.split(\"_\")[-1]  # Get \"1of3\"\n",
    "                return int(number_part.split(\"of\")[0])  # Get \"1\"\n",
    "            except:\n",
    "                return 0\n",
    "        return 0\n",
    "    \n",
    "    chunk_files.sort(key=get_chunk_number)\n",
    "    \n",
    "    print(f\"Rejoining {len(chunk_files)} chunks for {base_name}\")\n",
    "    \n",
    "    # Rejoin files\n",
    "    output_file = directory / f\"{base_name}.hif\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for chunk_file in chunk_files:\n",
    "            print(f\"  Adding: {chunk_file.name}\")\n",
    "            with open(chunk_file, 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "    \n",
    "    print(f\"  Rejoined file created: {output_file}\")\n",
    "    final_size = get_file_size_mb(output_file)\n",
    "    print(f\"  Final size: {final_size:.2f}MB\")\n",
    "\n",
    "    # Delete the individual chunk files\n",
    "    print(\"  Cleaning up chunk files:\")\n",
    "    for chunk_file in chunk_files:\n",
    "        try:\n",
    "            chunk_file.unlink()\n",
    "            print(f\"    Deleted: {chunk_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error deleting {chunk_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"  Cleanup complete - removed {len(chunk_files)} chunk files\")\n",
    "\n",
    "def validate_hif_schema(file_path):\n",
    "    url = \"https://raw.githubusercontent.com/pszufe/HIF-standard/main/schemas/hif_schema.json\"\n",
    "    schema = requests.get(url).json()\n",
    "    validator = fastjsonschema.compile(schema)\n",
    "    hiftext = json.load(open(file_path,'r'))\n",
    "    try:\n",
    "      validator(hiftext)\n",
    "      print(\"HIF-Compliant JSON.\")\n",
    "    except Exception as e:\n",
    "       print(f\"Invalid JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a36fa719-5669-437b-a58c-da8680199af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File <_io.BufferedWriter name='/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down_1of5.hif'> downloaded successfully\n",
      "File <_io.BufferedWriter name='/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down_2of5.hif'> downloaded successfully\n",
      "File <_io.BufferedWriter name='/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down_3of5.hif'> downloaded successfully\n",
      "File <_io.BufferedWriter name='/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down_4of5.hif'> downloaded successfully\n",
      "File <_io.BufferedWriter name='/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down_5of5.hif'> downloaded successfully\n",
      "Rejoining 5 chunks for Allen_Brain_Atlas_down\n",
      "  Adding: Allen_Brain_Atlas_down_1of5.hif\n",
      "  Adding: Allen_Brain_Atlas_down_2of5.hif\n",
      "  Adding: Allen_Brain_Atlas_down_3of5.hif\n",
      "  Adding: Allen_Brain_Atlas_down_4of5.hif\n",
      "  Adding: Allen_Brain_Atlas_down_5of5.hif\n",
      "  Rejoined file created: /nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download/Allen_Brain_Atlas_down.hif\n",
      "  Final size: 172.64MB\n",
      "  Cleaning up chunk files:\n",
      "    Deleted: Allen_Brain_Atlas_down_1of5.hif\n",
      "    Deleted: Allen_Brain_Atlas_down_2of5.hif\n",
      "    Deleted: Allen_Brain_Atlas_down_3of5.hif\n",
      "    Deleted: Allen_Brain_Atlas_down_4of5.hif\n",
      "    Deleted: Allen_Brain_Atlas_down_5of5.hif\n",
      "  Cleanup complete - removed 5 chunk files\n",
      "HIF-Compliant JSON.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Allen_Brain_Atlas_down\"\n",
    "download_path = \"/nfs/turbo/umms-indikar/Joshua/HIF-datasets/test_download\"\n",
    "download_hif_dataset(dataset_name, download_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
